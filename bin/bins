#!/usr/bin/python3

import ssl
import yaml
import json
import logging
import requests
import argparse
from datetime import datetime
from bs4 import BeautifulSoup
from http.server import HTTPServer, BaseHTTPRequestHandler

class BinHandler(BaseHTTPRequestHandler):

    def do_GET(self):
        if self.path == '/':

            logging.info('request recieved')
            try:
                response = get_bin_html(self.html_template, self.styling, self.config)
                self.send_response(200)
            except Exception as exception:
                response = 'internal server error'
                logging.exception('internal server error, %s', exception)
                self.send_response(500)

            logging.info('sending response')

            self.protocol_version = 'HTTP/1.0'
            self.send_header("Content-type", "text/html")
            self.end_headers()
            response_bytes = response.encode()
            self.send_header("Content-length", response_bytes)

            self.wfile.write(response_bytes)


def load_config(filepath):

    with open(filepath, 'r') as file:
        config = yaml.safe_load(file)

    return config


def get_address_id(url, address):

    params = { 'postCode': address['postcode'] }
    tcp_response = requests.get(url=url, params=params, timeout=10)
    response = json.loads(tcp_response.text)

    for address_resp in response:
        if address_resp.get('street') == address['street'].upper() and address_resp.get('houseNumber') == address['number']:
            return address_resp['id']


def get_bin_dates(url, lookahead=12):

    params = {'numberOfCollections': lookahead}
    tcp_response = requests.get(url=url, params=params, timeout=10)
    response = json.loads(tcp_response.text)

    return response['collections']


def generate_bin_rota (new_bin_dates, config):
    rooms = config['rooms']

    logging.info('loading historic data...')
    with open(config['data_file'], 'r') as file:
        bin_data = json.load(file)
    
    logging.info('processing data...')
    earliest_new_date = min( item['date'] for item in new_bin_dates )
    bin_data = [ entry for entry in bin_data if entry['date'] < earliest_new_date ] 

    last_room = bin_data[-1]['room']
    last_room_index = rooms.index(last_room)

    for bin_date in new_bin_dates:
        is_valid = set(bin_date['roundTypes']).intersection(config['valid_bins'])
        if not bin_date.get('room') and is_valid:
            last_room_index += 1
            bin_date['room'] = rooms[last_room_index%len(rooms)]
            bin_date['roundTypes'] = list(is_valid)
            bin_data.append(bin_date)

    bin_data = sorted(bin_data, key=lambda x:x['date'])

    logging.info('writing data to file...')

    with open(config['data_file'], 'w') as file:
        json.dump(bin_data, file, indent=4)

    return bin_data
        

def make_table(bin_rota, soup):

    table = soup.new_tag('table', attrs={'class': 'main-body'})

    for item in bin_rota:
        row = soup.new_tag('tr')

        date_col = soup.new_tag('td')
        date_col.string = datetime.strptime(item['date'],'%Y-%m-%dT%H:%M:%SZ').strftime('%d-%b')

        bin_types = soup.new_tag('td', attrs={'class': item['roundTypes'][0].lower()})
        bin_types.string = ', '.join([round_type.title() for round_type in item['roundTypes']])

        room_col = soup.new_tag('td')
        room_col.string = 'Room ' + item['room'].title()

        moved_col = soup.new_tag('td')
        moved_col.string = '(moved)' if item['slippedCollection'] else ''

        row.append(date_col)
        row.append(bin_types)
        row.append(room_col)
        row.append(moved_col)

        table.append(row)
    return table


def get_bin_html(html, styling, config):
    address_api_url = config['gov_api_address_url'] 
    collection_api_url = config['gov_api_collection_url'] 

    logging.info('fetching address id...')
    address_id = get_address_id(address_api_url, config['address'])
    if not address_id:
        logging.error('error, no address id returned')
        return 'could not locate address'

    collection_api_url += f'/{address_id}'

    logging.info('fetching bin json...')
    new_bin_dates = get_bin_dates(collection_api_url, config['lookahead'])

    bin_rota = generate_bin_rota(new_bin_dates, config)

    logging.info('generating html...')
    soup = BeautifulSoup(html, 'html.parser')
    table = make_table(bin_rota, soup) 
    soup.body.append(table)
    soup.style.append(styling)

    return str(soup)


def run_server(server_class, handler_class, config):

    logging.info('loading html and css')
    with open(config['html_template'], 'r') as file:
        html = file.read()

    with open(config['style_path'], 'r') as file:
        styling = file.read()

    server_address = ('', config['server_port'])
    httpd = server_class(server_address, handler_class)

    httpd.RequestHandlerClass.config = config
    httpd.RequestHandlerClass.html_template = html 
    httpd.RequestHandlerClass.styling = styling 

    ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
    ssl_context.load_cert_chain(certfile=config['cert_file'], keyfile=config['privkey_file'])
    httpd.socket = ssl_context.wrap_socket(httpd.socket, server_side=True)

    logging.info('starting server')
    httpd.serve_forever()


parser = argparse.ArgumentParser(description='Get data of bin dates from location, input a config file')
parser.add_argument('config_path')
args = parser.parse_args()

config_opts = load_config(args.config_path)

logging.basicConfig(filename=config_opts['log_path'], format='%(asctime)s - %(message)s', level=logging.INFO)
logging.info('loaded config')

run_server(HTTPServer, BinHandler, config_opts)
